{
    "run_name": "gpt2-medium-wikitext",
    "dataset": {
        "path": "wikitext",
        "name": "wikitext-103-raw-v1",
        "num_proc": 20
    },
    "model_name": "gpt2-medium-wikitext",
    "train_files": [],
    "max_train_samples": null,
    "max_eval_samples": null,
    "max_steps": 10,
    "model_type": "gpt2",
    "bf16": true,
    "streaming": true,
    "batch_size": 40000,
    "validation_split_percentage": "1",
    "device_map": "auto",
    "cache_dir": "./output/models/gpt2-medium-wikitext",
    "model_revision": "zero",
    "use_fast_tokenizer": true,
    "tokenizer_file": "gpt2-medium",
    "output_dir": "./output/models/gpt2-medium-wikitext",
    "overwrite_cache": false,
    "should_log": true,
    "seed": 42,
    "use_cpu": false,
    "data_seed": 43,
    "num_train_epochs": 2,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "max_seq_length": 8192,
    "block_size": 8192,
    "learning_rate": 1e-4,
    "push_to_hub": true,
    "hub_model_id": "BluebrainAI/gpt2-medium-wikitext",
    "weight_decay": 0.1,
    "torch_empty_cache_steps": 100,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 1,
    "do_train": true,
    "logging_steps": 2,
    "eval_steps": 2,
    "do_eval": true,
    "eval_strategy": "steps",
    "gradient_accumulation_steps": 2,
    "save_total_limit": 2,
    "overwrite_output_dir": true,
    "save_strategy": "steps",
    "report_to": "wandb",
    "gradient_checkpointing": true,
    "keep_in_memory": true
}