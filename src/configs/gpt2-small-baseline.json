{
    "run_name": "llama-cosmopedia-drift-sigmoid-2B-base",
    "dataset": {
        "path": "HuggingFaceTB/smollm-corpus",
        "name": "cosmopedia-v2",
        "split": "train",
        "num_proc": 20
    },
    "model_name": "llama-cosmopedia-drift-sigmoid-2B-base",
    "train_files": [],
    "max_train_samples": null,
    "max_eval_samples": null,
    "max_steps": 10,
    "model_type": "llama",
    "bf16": true,
    "streaming": true,
    "batch_size": 40000,
    "validation_split_percentage": "1",
    "device_map": "auto",
    "cache_dir": "./output/models/llama-cosmopedia-drift-sigmoid-2B-base",
    "model_revision": "zero",
    "use_fast_tokenizer": true,
    "tokenizer_file": "meta-llama/Llama-3.2-1B",
    "output_dir": "./output/models/llama-cosmopedia-drift-sigmoid-2B-base",
    "overwrite_cache": false,
    "should_log": true,
    "seed": 42,
    "use_cpu": false,
    "data_seed": 43,
    "num_train_epochs": 1,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "max_seq_length": 8192,
    "block_size": 8192,
    "learning_rate": 1e-4,
    "push_to_hub": true,
    "hub_model_id": "BluebrainAI/shivanand-llama-cosmopedia-drift-sigmoid-2B-base",
    "weight_decay": 0.1,
    "torch_empty_cache_steps": 100,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 1,
    "do_train": true,
    "logging_steps": 2,
    "eval_steps": 2,
    "do_eval": true,
    "eval_strategy": "steps",
    "gradient_accumulation_steps": 2,
    "save_total_limit": 2,
    "overwrite_output_dir": true,
    "save_strategy": "steps",
    "report_to": "wandb",
    "gradient_checkpointing": true,
    "baseline_each_head": true,
    "only_drift": true,
    "only_diffusion": false,
    "attention_type": "eager",
    "keep_in_memory": true,
    "deepspeed": "src/config/ds_config_zero1.json",
    "llama_model_config": {
        "vocab_size": 128256,
        "attention_bias": false,
        "attention_dropout": 0.0,
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "head_dim": 64,
        "hidden_act": "silu",
        "hidden_size": 2048,
        "initializer_range": 0.02,
        "intermediate_size": 8192,
        "max_position_embeddings": 8192,
        "mlp_bias": false,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 16,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": null,
        "rope_theta": 500000.0,
        "tie_word_embeddings": true,
        "torch_dtype": "bfloat16",
        "use_cache": true
    }
}