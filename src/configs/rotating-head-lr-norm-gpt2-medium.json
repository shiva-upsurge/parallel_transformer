{
    "run_name": "rotating-head-lr-norm-gpt2-medium-wikitext",
    "dataset": {
        "path": "wikitext",
        "name": "wikitext-103-raw-v1",
        "num_proc": 20
    },
    "model_name": "rotating-head-lr-norm-gpt2-medium-wikitext",
    "train_files": [],
    "max_train_samples": null,
    "max_eval_samples": null,
    "model_type": "rotating-head-gpt2",
    "rotatinghead": "lr",
    "include_for_metrics":["loss"],
    "bf16": true,
    "streaming": false,
    "batch_size": 40000,
    "validation_split_percentage": "1",
    "device_map": "auto",
    "cache_dir": "./output/models/rotating-head-lr-norm-gpt2-medium-wikitext",
    "model_revision": "zero",
    "use_fast_tokenizer": true,
    "tokenizer_file": "gpt2-medium",
    "output_dir": "./output/models/rotating-head-lr-norm-gpt2-medium-wikitext",
    "overwrite_cache": false,
    "should_log": true,
    "seed": 42,
    "use_cpu": false,
    "data_seed": 43,
    "num_train_epochs": 5,
    "per_device_train_batch_size": 64,
    "per_device_eval_batch_size": 64,
    "max_seq_length": 1024,
    "block_size": 1024,
    "learning_rate": 1e-4,
    "push_to_hub": true,
    "hub_model_id": "BluebrainAI/rotating-head-lr-norm-gpt2-medium-wikitext",
    "weight_decay": 0.01,
    "torch_empty_cache_steps": 100,
    "lr_scheduler_type": "linear",
    "warmup_ratio": 0.1,
    "do_train": true,
    "logging_steps": 100,
    "eval_steps": 500,
    "save_steps":500,
    "do_eval": true,
    "eval_strategy": "steps",
    "gradient_accumulation_steps": 1,
    "save_total_limit": 5,
    "overwrite_output_dir": true,
    "save_strategy": "steps",
    "report_to": "wandb",
    "gradient_checkpointing": true,
    "keep_in_memory": true,
    "metric_for_best_model":"eval_loss",
    "greater_is_better":false
}